apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-deployment
spec:
  replicas: {{.Values.replicaCount}}
  selector:
    matchLabels:
      app: llm-api
  # the mold of deployment - how to create pods
  template:
    metadata:
      labels:
        app: llm-api
    spec:
      containers:
        - name: llm-container
          image: "{{.Values.image.repository}}:{{.Values.image.tag}}"
          imagePullPolicy: "{{.Values.image.pullPolicy}}"

          ports:
            - containerPort: {{.Values.service.port}}

          env:
            - name: HARDWARE
              value: "{{.Values.hardware}}"

          # --- DYNAMIC RESOURCES ---
          # If hardware is GPU, request NVIDIA GPU. 
          # If CPU, request standard RAM/CPU.
          resources:
            {{- if eq .Values.hardware "gpu"}}
            limits:
              nvidia.com/gpu: 1 # Request 1 GPU
            {{- else}}
            limits:
              cpu: "2"
              memory: "4Gi"
            {{- end}}
          # --- VOLUME MOUNT ---
          # This plugs the model folder into the container
          volumeMounts:
            - name: model-storage
              mountPath: "{{.Values.containerModelPath}}"
      volumes:
        - name: model-storage
          hostPath:
          # Maps to the path inside the Minikube VM
            path: "{{.Values.hostModelPath}}"
            type: DirectoryOrCreate
          
